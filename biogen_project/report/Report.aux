\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\vspace *{13.mm}}
\@writefile{toc}{\contentsline {chapter}{\bfseries  {Abstract}}{v}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\vspace *{10mm}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{hernan2023}
\citation{hernan2023}
\citation{hernan2023}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Causal inference from complex longitudinal data}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methods}{{2}{3}{Causal inference from complex longitudinal data}{chapter.2}{}}
\@writefile{brf}{\backcite{hernan2023}{{3}{2}{chapter.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}TIME-VARYING TREATMENTS (19)}{3}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The causal effect of time-varying treatments (19.1)}{3}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Treatment strategies (19.2)}{3}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Sequentially randomized experiments (19.3)}{4}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Causal diagram of a randomized clinical trial with dynamic treatment strategy, meaning that depending on some of the patients covariates, the treatment may be changed. Corresponds to Figure 19.2 in \cite  {hernan2023}\relax }}{4}{figure.caption.4}\protected@file@percent }
\@writefile{brf}{\backcite{hernan2023}{{4}{2.1}{figure.caption.4}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:19.1}{{2.1}{4}{Causal diagram of a randomized clinical trial with dynamic treatment strategy, meaning that depending on some of the patients covariates, the treatment may be changed. Corresponds to Figure 19.2 in \cite {hernan2023}\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Sequential exchangeability (19.4)}{4}{subsection.2.1.4}\protected@file@percent }
\newlabel{eq:seqexch}{{2.1}{4}{Sequential exchangeability (19.4)}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Identifiability under some but not all treatment strategies (19.5)}{4}{subsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Time-varying confounding and time-varying confounders (19.6)}{4}{subsection.2.1.6}\protected@file@percent }
\citation{hernan2023}
\citation{hernan2023}
\citation{hernan2023}
\citation{hernan2023}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}TREATMENT-CONFOUNDER FEEDBACK (20)}{5}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The elements of treatment-confounder feedback (20.1)}{5}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Corresponds to Figure 19.2 or 20.1 in \cite  {hernan2023}. The \textbf  {treatment-confounder feedback} means in this example that the confounder ($L_k$) affects the treatment ($A_k$) \texttt  {and} the treatment ($A_k$) affects in-turn again the confounder ($L_{k+1}$). And this \textbf  {treatment-confounder feedback} can be caused directly or even go over unmeasured variables $U_k$. Remark: When the two red edges would be removed, we would still have time-varying confounders but not anymore treatment-confounder feedback. \relax }}{5}{figure.caption.5}\protected@file@percent }
\@writefile{brf}{\backcite{hernan2023}{{5}{2.2}{figure.caption.5}}}
\newlabel{fig:20.1}{{2.2}{5}{Corresponds to Figure 19.2 or 20.1 in \cite {hernan2023}. The \textbf {treatment-confounder feedback} means in this example that the confounder ($L_k$) affects the treatment ($A_k$) \texttt {and} the treatment ($A_k$) affects in-turn again the confounder ($L_{k+1}$). And this \textbf {treatment-confounder feedback} can be caused directly or even go over unmeasured variables $U_k$. Remark: When the two red edges would be removed, we would still have time-varying confounders but not anymore treatment-confounder feedback. \relax }{figure.caption.5}{}}
\citation{hernan2023}
\citation{hernan2023}
\@writefile{toc}{\contentsline {subsubsection}{An example why traditional methods fail}{6}{section*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Corresponds to Figure 20.3 in \cite  {hernan2023}. Illustrates a sequentially randomized trial with two time-points $k=0,1$. Furthermore, treatment $A_0$ is unconditionally (or marginally) randomized and $A_1$ is randomized only with respect to confounders $L_1$. Since no edge from $A_1, L_1, A_1$ to $Y$ is drawn, there is no effect assumed. \relax }}{6}{figure.caption.7}\protected@file@percent }
\@writefile{brf}{\backcite{hernan2023}{{6}{2.3}{figure.caption.7}}}
\newlabel{fig:20.3}{{2.3}{6}{Corresponds to Figure 20.3 in \cite {hernan2023}. Illustrates a sequentially randomized trial with two time-points $k=0,1$. Furthermore, treatment $A_0$ is unconditionally (or marginally) randomized and $A_1$ is randomized only with respect to confounders $L_1$. Since no edge from $A_1, L_1, A_1$ to $Y$ is drawn, there is no effect assumed. \relax }{figure.caption.7}{}}
\newlabel{eq:20.3}{{2.2}{6}{An example why traditional methods fail}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The bias of traditional methods (20.2)}{6}{subsection.2.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Table 20.1 in \cite  {hernan2023}\relax }}{6}{table.caption.8}\protected@file@percent }
\@writefile{brf}{\backcite{hernan2023}{{6}{2.1}{table.caption.8}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Why traditional methods fail (20.3)}{7}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}G-METHODS FOR TIME-VARYING TREATMENTS (21)}{7}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The g-formula for time-varying treatments (21.1)}{7}{subsection.2.3.1}\protected@file@percent }
\citation{Hothorn2020}
\citation{Hothorn2020}
\citation{Hothorn2020}
\citation{Hothorn2017}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Transformation models}{8}{section.2.4}\protected@file@percent }
\newlabel{sec:trans_model}{{2.4}{8}{Transformation models}{section.2.4}{}}
\@writefile{brf}{\backcite{Hothorn2020}{{8}{2.4}{section.2.4}}}
\newlabel{eq:lspara}{{2.3}{8}{Transformation models}{equation.2.4.3}{}}
\newlabel{eq:ll_trans}{{2.4}{8}{Transformation models}{equation.2.4.4}{}}
\newlabel{eq:gentram}{{2.5}{8}{Transformation models}{equation.2.4.5}{}}
\@writefile{brf}{\backcite{Hothorn2020}{{8}{2.4}{equation.2.4.5}}}
\@writefile{brf}{\backcite{Hothorn2020}{{8}{2.4}{equation.2.4.5}}}
\@writefile{brf}{\backcite{Hothorn2017}{{8}{2.4}{equation.2.4.5}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}An example}{9}{subsection.2.4.1}\protected@file@percent }
\newlabel{sec:example}{{2.4.1}{9}{An example}{subsection.2.4.1}{}}
\newlabel{eq:simplemodel}{{2.6}{9}{An example}{equation.2.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Impact of collinearity on the instability of estimates. \relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig:cond_ill}{{2.4}{9}{Impact of collinearity on the instability of estimates. \relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Variance decomposition matrices as introduced by Belsley in the first row and summary output of the multiple linear regression models on the second row. Left side corresponds to the example with higher collinearity and the right table for the lower.\relax }}{10}{table.caption.10}\protected@file@percent }
\newlabel{tab:vdm1}{{2.2}{10}{Variance decomposition matrices as introduced by Belsley in the first row and summary output of the multiple linear regression models on the second row. Left side corresponds to the example with higher collinearity and the right table for the lower.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Univariate fitted model (\texttt  {y$\sim $x1}) of the same data sets as in Figure~\ref {fig:cond_ill}. The slope of the line represents ${\boldsymbol  {\hat  \beta }}[1]$ which would be truly 2 and the confidence interval thereof is given in the box. Obviously, only the right plot with low collinearity seems to capture the true effect whereas with higher collinearity the estimate is biased.\relax }}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig:coll2}{{2.5}{10}{Univariate fitted model (\texttt {y$\sim $x1}) of the same data sets as in Figure~\ref {fig:cond_ill}. The slope of the line represents $\hbbeta [1]$ which would be truly 2 and the confidence interval thereof is given in the box. Obviously, only the right plot with low collinearity seems to capture the true effect whereas with higher collinearity the estimate is biased.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Differences between \texttt  {lm} and \texttt  {tram::Lm}}{10}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Maximum-Likelihood estimation for the linear regression model}{11}{subsection.2.5.1}\protected@file@percent }
\newlabel{sec:mlnlm}{{2.5.1}{11}{Maximum-Likelihood estimation for the linear regression model}{subsection.2.5.1}{}}
\newlabel{eq:ll_ls}{{2.7}{11}{Maximum-Likelihood estimation for the linear regression model}{equation.2.5.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Maximum-Likelihood estimation for the transformation model equivalent (\texttt  {tram::Lm})}{11}{subsection.2.5.2}\protected@file@percent }
\newlabel{sec:mltramLM}{{2.5.2}{11}{Maximum-Likelihood estimation for the transformation model equivalent (\texttt {tram::Lm})}{subsection.2.5.2}{}}
\newlabel{eq:ll_tramLm}{{2.8}{11}{Maximum-Likelihood estimation for the transformation model equivalent (\texttt {tram::Lm})}{equation.2.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Simulating data as ${\textbf  {\textit  {y}}}=10+2{\textbf  {\textit  {x}}}_1+2{\textbf  {\textit  {x}}}_2+s_y\cdot {\boldsymbol  {\varepsilon }}$ with $\left ({\textbf  {\textit  {x}}}_1,{\textbf  {\textit  {x}}}_2,{\boldsymbol  {\varepsilon }}\right )\sim \N  _{3n}(0,1),n=100$. The scaling factor $s_y$ is iterated on a grid between 0.3333333 and 100 where a low scaling factor means that the outcome ${\textbf  {\textit  {y}}}$ is well explainable and thus collinearity for \texttt  {tram::Lm} is higher. Wald statistics are plotted restricted to have maximum values of 20 and points laying above are illustrated as triangles.\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:colllikelihood}{{2.6}{12}{Simulating data as $\y =10+2\x _1+2\x _2+s_y\cdot \bvarepsilon $ with $\left (\x _1,\x _2,\bvarepsilon \right )\sim \N _{3n}(0,1),n=100$. The scaling factor $s_y$ is iterated on a grid between 0.3333333 and 100 where a low scaling factor means that the outcome $\y $ is well explainable and thus collinearity for \texttt {tram::Lm} is higher. Wald statistics are plotted restricted to have maximum values of 20 and points laying above are illustrated as triangles.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{13}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Approximate likelihood}{13}{section.A.1}\protected@file@percent }
\newlabel{sec:approxlikelihood}{{A.1}{13}{Approximate likelihood}{section.A.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Computational reproducibility}{14}{section.A.2}\protected@file@percent }
\newlabel{sec:repro}{{A.2}{14}{Computational reproducibility}{section.A.2}{}}
\bibstyle{mywiley}
\bibdata{biblio}
\bibcite{hernan2023}{{1}{2023}{{Hernan and Robins}}{{}}}
\bibcite{Hothorn2020}{{2}{2020}{{Hothorn}}{{}}}
\bibcite{Hothorn2017}{{3}{2017}{{Hothorn {\it  et~al.}}}{{}}}
\@writefile{toc}{\vspace *{10mm}}
\@writefile{toc}{\contentsline {chapter}{\bfseries  Bibliography}{17}{section*.13}\protected@file@percent }
\gdef \@abspage@last{28}
