% LaTeX file for Chapter 02
<<'preamble02',include=FALSE, cache=FALSE >>=
library(knitr)
library(scatterplot3d); library(scales); library(mvtnorm);library(Collinearity)
library(tableone);library(biostatUZH);library(xtable);library(RColorBrewer)
library(tidyverse);library(tram);library(survival)

opts_chunk$set( 
    fig.path='figure/ch02_fig',    
    self.contained=FALSE,
    cache=F,#reduced time if TRUE
    dev = "pdf" # reduced size when "png" !
) 
@

\chapter{Causal inference from complex longitudinal data}\label{chap:methods}

This chapter summarizes \cite{hernan2023} part III.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TIME-VARYING TREATMENTS (19)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%
\subsection{The causal effect of time-varying treatments (19.1)}
% %%%%%%%%%%%%%%%%


time-varying dichotomous treatment $A_k$ at time $k=0,1,2,\dots,K$

We refer to the treatment history as $\bar{A}_k=\left(A_0,\dots, A_K\right)$
Patients under treatment throughout the whole time $K$ have $\bar{A}=\left(A_0=1,\dots, A_K=1\right)=\bar{1}$ while patients that never receive treatment do have history $\bar{A}=\left(A_0=0,\dots, A_K=0\right)=\bar{0}$.
Most likely patients will have some sort of a mixture between those two treatment strategies due to various reasons and therefore a compact (and true) notation is not really possible.
The average treatment effect at timepoint $k$ may be defined as:

$$\E\left[Y^{a_k=1}\right]-\E\left[Y^{a_k=0}\right]$$

however, this does not reflect the overall goal to quantify the average causal effect of the time-varying treatment.
Due to the fact that in reality the treatment strategies cannot be completely discriminated between $\bar{1}$ or $\bar{0}$, no \textbf{unique} average causal effect is defined.

% %%%%%%%%%%%%%%%%
\subsection{Treatment strategies (19.2)}
% %%%%%%%%%%%%%%%%

"Always treat" vs. "Never treat": $\E\left[Y^{\bar{a}=\bar{1}}\right]-\E\left[Y^{\bar{a}=\bar{0}}\right]$

Dynamic treatment strategy: $a_k$ depends on the evolution of the patients time-varying covariate(s) $\bar{L}_k$ (otherwise they are called static). Development of ADA and subsequent discontinuation of the treatment is a form of dynamic treatment strategy.


The average causal effect estimate of a (potentially) time-varying treatment is only well defined if the strategies are specified (and hold). Since at each timepoint $k$ there is the option (random or deterministic) to treat or not to treat, there are a many different treatment strategies available resulting in \textbf{many} different causal effects. 

% %%%%%%%%%%%%%%%%
\subsection{Sequentially randomized experiments (19.3)}
% %%%%%%%%%%%%%%%%

%\newpage
\tikzstyle{block} = [draw=black, thick, text width=2cm, minimum height=1cm, align=center]
\tikzstyle{cloud} = [draw=black, ellipse, text width=1cm, minimum height=1cm, align=center] 
% \tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
%     minimum height=2em]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{figure}[H]
\centering
\begin{tikzpicture}

    \node[cloud] (L0) {$L_0$};
    \node[cloud, right=of L0, xshift=-0.2cm] (A0) {$A_0$};
    \node[cloud, right=of A0, xshift=0.9cm] (L1) {$L_1$};
    \node[cloud, right=of L1, xshift=-0.2cm] (A1) {$A_1$};
    
    % outcome
    \node[cloud, right=of A1, xshift=0.9cm] (Y) {$Y$};
    
    % unobserved confounders
    \node[cloud, below=of L0, yshift=-0.7cm] (U0) {$U_0$};
    \node[cloud, below=of L1, yshift=-0.7cm] (U1) {$U_1$};
    
    % edges
    \draw [arrow] (L0) -- (A0);
    \draw [arrow] (L0) to [out=45,in=135]  (L1);
    \draw [arrow] (L0) to [out=45,in=135]  (A1);
    \draw [arrow] (L0) to [out=45,in=135]  (Y);
    
    
    \draw [arrow] (A0) -- (L1);
    \draw [arrow] (A0) -- (U1);
    \draw [arrow] (A0) to [out=45,in=150]  (A1);
    \draw [arrow] (A0) to [out=45,in=150]  (Y);
    
    
    \draw [arrow] (L1) -- (A1);
    \draw [arrow] (L1) to [out=45,in=160]  (Y);
    
    \draw [arrow] (A1) -- (Y);
    
    \draw [arrow] (U0) -- (L0);
    \draw [arrow] (U0) -- (L1);
    \draw [arrow] (U0) -- (U1);
    \draw [arrow] (U0) to [out=-45,in=-90]  (Y);
    
    \draw [arrow] (U1) -- (L1);
    \draw [arrow] (U1) -- (Y);

    
\end{tikzpicture}
\caption{Causal diagram of a randomized clinical trial with dynamic treatment strategy, meaning that depending on some of the patients covariates, the treatment may be changed. Corresponds to Figure 19.2 in \cite{hernan2023}}\label{fig:19.1}
\end{figure}

% %%%%%%%%%%%%%%%%
\subsection{Sequential exchangeability (19.4)}
% %%%%%%%%%%%%%%%%

Causal inference (unbiased effect estimation) with time-fixed treatment effects require conditional exchangeability $Y^a \perp A\mid L$ 

(\textcolor{red}{p247: "Conditional exchangeability holds in observational studies if the probability of receiving treatment depends on the measured covariates L and, conditional on L, does not further depend on any unmeasured, common causes of treatment and outcome."})

Sequential conditional exchangeability:
\begin{align}
Y^g \perp A_k\mid \bar{A}_{k-1}=g\left(\bar{A}_{k-2}, \bar{L}_{k-1}\right),\bar{L}_k\label{eq:seqexch}
\end{align}

% %%%%%%%%%%%%%%%%
\subsection{Identifiability under some but not all treatment strategies (19.5)}
% %%%%%%%%%%%%%%%%

\textcolor{red}{Needs knowledge of $d$-separation and SWIG.}


% %%%%%%%%%%%%%%%%
\subsection{Time-varying confounding and time-varying confounders (19.6)}
% %%%%%%%%%%%%%%%%

p253: "\textit{Achieving approximate exchangeability requires expert knowledge,
which will guide investigators in the design of their studies to measure as
many of the relevant variables $\bar{L}_k$ as possible. For example, in an HIV study, experts would agree that time-varying variables like CD4 cell count, viral load, symptoms need to be appropriately measured and adjusted for.}"

p253: "\textit{But the question "Are the measured covariates sufficient to ensure sequential exchangeability?" can never be answered with certainty.}"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TREATMENT-CONFOUNDER FEEDBACK (20)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

"\textit{Suppose that we have a study in which the strongest form of sequential exchangeability holds:
the measured time-varying confounders are sufficient to validly estimate the causal effect of any treatment strategy.
Then the question is what confounding adjustment method to use.
The answer to this question highlights a key problem in causal inference about time-varying treatments: treatment-confounder feedback.}"

If treatment-confounder feedback is apparent, traditional adjustment methods induce bias.

% %%%%%%%%%%%%%%%%
\subsection{The elements of treatment-confounder feedback (20.1)}
% %%%%%%%%%%%%%%%%



\begin{figure}[H]
\centering
\begin{tikzpicture}

    \node[cloud] (L0) {$L_0$};
    \node[cloud, right=of L0, xshift=-0.2cm] (A0) {$A_0$};
    \node[cloud, right=of A0, xshift=0.9cm] (L1) {$L_1$};
    \node[cloud, right=of L1, xshift=-0.2cm] (A1) {$A_1$};
    
    % outcome
    \node[cloud, right=of A1, xshift=0.9cm] (Y) {$Y$};
    
    % unobserved confounders
    \node[cloud, below=of L0, yshift=-0.7cm] (U0) {$U_0$};
    \node[cloud, below=of L1, yshift=-0.7cm] (U1) {$U_1$};
    
    % edges
    \draw [arrow] (L0) -- (A0);
    \draw [arrow] (L0) to [out=45,in=135]  (L1);
    \draw [arrow] (L0) to [out=45,in=135]  (A1);
    \draw [arrow] (L0) to [out=45,in=135]  (Y);
    
    
    \draw [arrow, red] (A0) -- (L1);
    \draw [arrow, red] (A0) -- (U1);
    \draw [arrow] (A0) to [out=45,in=150]  (A1);
    \draw [arrow] (A0) to [out=45,in=150]  (Y);
    
    
    \draw [arrow] (L1) -- (A1);
    \draw [arrow] (L1) to [out=45,in=160]  (Y);
    
    \draw [arrow] (A1) -- (Y);
    
    \draw [arrow] (U0) -- (L0);
    \draw [arrow] (U0) -- (L1);
    \draw [arrow] (U0) -- (U1);
    \draw [arrow] (U0) to [out=-45,in=-90]  (Y);
    
    \draw [arrow] (U1) -- (L1);
    \draw [arrow] (U1) -- (Y);

    
\end{tikzpicture}
\caption{Corresponds to Figure 19.2 or 20.1 in \cite{hernan2023}. 
The \textbf{treatment-confounder feedback} means in this example that the confounder ($L_k$) affects the treatment ($A_k$) \texttt{and} the treatment ($A_k$) affects in-turn again the confounder ($L_{k+1}$). And this \textbf{treatment-confounder feedback} can be caused directly or even go over unmeasured variables $U_k$.
Remark: When the two red edges would be removed, we would still have time-varying confounders but not anymore treatment-confounder feedback.
}\label{fig:20.1}
\end{figure}

% % %%%%%%%%%%%%%%
\subsubsection{An example why traditional methods fail}
% % %%%%%%%%%%%%%%

\begin{figure}[H]
\centering
\begin{tikzpicture}

    \node[cloud] (L1) {$L_1$};
    \node[cloud, right=of L1, xshift=-0.2cm] (A1) {$A_1$};
    \node[cloud, left=of L1, xshift=-0.9cm] (A0) {$A_0$};
    % outcome
    \node[cloud, right=of A1, xshift=0.9cm] (Y) {$Y$};
    
    % unobserved confounders
    \node[cloud, below=of L1, yshift=-0.7cm] (U1) {$U_1$};
    
    % edges
    \draw [arrow] (A0) -- (L1);

    \draw [arrow] (L1) -- (A1);

    \draw [arrow] (U1) -- (L1);
    \draw [arrow] (U1) -- (Y);

    
\end{tikzpicture}
\caption{Corresponds to Figure 20.3 in \cite{hernan2023}. 
Illustrates a sequentially randomized trial with two time-points $k=0,1$. Furthermore, treatment $A_0$ is unconditionally (or marginally) randomized and $A_1$ is randomized only with respect to confounders $L_1$. 
Since no edge from $A_1, L_1, A_1$ to $Y$ is drawn, there is no effect assumed.
}\label{fig:20.3}
\end{figure}

We are interested in estimating the \textbf{average causal treatment effect} of the static treatment strategy "always treat" ($a_0=1,a_1=1$) versus the static treatment strategy "never treat" ($a_0=0,a_1=0$) on the outcome $Y$. Thus the \textbf{average causal treatment effect} is 
\begin{align}
\E\left[Y^{a_0=1,a_1=1}\right]-\E\left[Y^{a_0=0,a_1=0}\right] \label{eq:20.3}
\end{align}
and should be 0 since there are no forward directed paths possible from $A_0$ or $A_1$ to $Y$. In addition, since there are no edges from unmeasured variables $U$ into the treatment $A$, we can argue Figure~\ref{fig:20.3} represents indeed a randomized trial and thus we should be able to use originating data ($A_0, L_1, A_1, Y$) to conclude \eqref{eq:20.3} is equals to 0.


Even though if sequential exchangeability is given (when conditioning on confounder $L_1$), if we have time-varying confounders \textbf{and} treatment-confounder feedback, traditional methods can not correctly adjust for those confounder and thereby induce bias.


% %%%%%%%%%%%%%%%%
\subsection{The bias of traditional methods (20.2)}
% %%%%%%%%%%%%%%%%

\begin{table}[H]
\caption{Table 20.1 in \cite{hernan2023}}
\centering
<<table, results = "asis", echo = FALSE, warning=FALSE,message=F>>=
library(tidyverse)
N <- c(2400, 1600, 2400, 9600, 4800, 3200, 1600, 6400)
A0 <- rep(c(0,1), each = 4)
L1 <- rep(c(0,0,1,1), times = 2)
A1 <- rep(c(0,1), times = 4)
mY <- rep(c(84,52,76,44), each = 2)
dcomp <- data.frame("N"=N, "A0"=A0, "L1"=L1, "A1"=A1, "my" = mY)

d <- data.frame("A0"=NA, "L1"=NA, "A1"=NA,"y"=NA)[-1,]
set.seed(122341)
for(i in 1:nrow(dcomp)){
  y <- rnorm(n=dcomp$N[i])
  y <- (y-mean(y))/sd(y)+dcomp$my[i]
  d <- rbind(d,data.frame("A0"=dcomp$A0[i], "L1"=dcomp$L1[i], "A1"=dcomp$A1[i],"y"=y))
}

dsum <- d %>% group_by(A0, L1, A1)%>%
  summarise("N" = n()
            ,"my" = mean(y) )

#library(tableone);library(xtable); library(stringr); library(dplyr)
colnames(dsum) <- c("$A_0$", "$L_1$","$A_1$","$N$","Mean $Y$")
xtab <- xtable( dsum)
digits(xtab) <- 0
print(xtab, showAllLevels = TRUE, booktabs = TRUE,size = "footnotesize",
      include.rownames=FALSE,include.colnames=TRUE, printToggle = FALSE, noSpaces = TRUE, floating = FALSE,
      digits=0,
      hline.after = c(-1,0,nrow(xtab)),
      sanitize.text.function=function(x){x})

@
\end{table}
\begin{itemize}
\item $\P\left(A_0=1\right)=0.5$
\item $\P\left(A_1=1\mid L_1=0\right)=0.4$ and $\P\left(A_1=1\mid L_1=1\right)=0.8$
\end{itemize}


We can confirm the causal effects of $A_0$ and $A_1$ (conditinal on the past) are indeed zero when we treat $A_0$ and $A_1$ seperately as time-fixed treatments:
\begin{enumerate}
\item The average causal effect in the stratum $(A_0=0,L_1=0)$:
\begin{align*}
\E\left[Y^{a_1=1}\mid A_0=0,L_1=0\right]-\E\left[Y^{a_1=0}\mid A_0=0,L_1=0\right]=84-84=0
\end{align*}
\item The average causal effect in the stratum $(A_0=0,L_1=1)$:
\begin{align*}
\E\left[Y^{a_1=1}\mid A_0=0,L_1=1\right]-\E\left[Y^{a_1=0}\mid A_0=0,L_1=1\right]=52-52=0
\end{align*}
\item The average causal effect in the stratum $(A_0=1,L_1=0)$:
\begin{align*}
\E\left[Y^{a_1=1}\mid A_0=1,L_1=0\right]-\E\left[Y^{a_1=0}\mid A_0=1,L_1=0\right]=76-76=0
\end{align*}
\item The average causal effect in the stratum $(A_0=1,L_1=1)$:
\begin{align*}
\E\left[Y^{a_1=1}\mid A_0=1,L_1=1\right]-\E\left[Y^{a_1=0}\mid A_0=1,L_1=1\right]=44-44=0
\end{align*}
\end{enumerate}

Joining $A_0$ and $A_1$ as a joint time-varying treatment is not so easy even though the identifiablity conditions (\textcolor{red}{???}) hold and thus the data should be well enough to estimate the effect.
Comparing the two treatment strategies "always treat" versus "never treat":
\begin{align*}
\E\left[Y^{a_0=1,a_1=1}\right]-\E\left[Y^{a_0=0,a_1=0}\right]
\end{align*}
should be equals zero according to the $g$-null theorem (\textcolor{red}{???}).

<<echo=F>>=
# 1. no adjustment for confounding
never_ind <- dsum$`$A_0$`== 0 & dsum$`$A_1$`==0
ever_ind <- dsum$`$A_0$`== 1 & dsum$`$A_1$`==1

never <- weighted.mean(x = dsum$`Mean $Y$`[never_ind], w = dsum$`$N$`[never_ind])
ever <- weighted.mean(x = dsum$`Mean $Y$`[ever_ind], w = dsum$`$N$`[ever_ind])

# 2. strata L1=0
never_ind <- dsum$`$A_0$`== 0 & dsum$`$A_1$`==0 & dsum$`$L_1$`==0
ever_ind <- dsum$`$A_0$`== 1 & dsum$`$A_1$`==1 & dsum$`$L_1$`==0

never_L1eq0 <- weighted.mean(x = dsum$`Mean $Y$`[never_ind], w = dsum$`$N$`[never_ind])
ever_L1eq0 <- weighted.mean(x = dsum$`Mean $Y$`[ever_ind], w = dsum$`$N$`[ever_ind])

# 2. strata L1=1
never_ind <- dsum$`$A_0$`== 0 & dsum$`$A_1$`==0 & dsum$`$L_1$`==1
ever_ind <- dsum$`$A_0$`== 1 & dsum$`$A_1$`==1 & dsum$`$L_1$`==1

never_L1eq1 <- weighted.mean(x = dsum$`Mean $Y$`[never_ind], w = dsum$`$N$`[never_ind])
ever_L1eq1 <- weighted.mean(x = dsum$`Mean $Y$`[ever_ind], w = dsum$`$N$`[ever_ind])
@

With the classical methods this is however not the case:
\begin{align*}
\E\left[Y\mid A_0=1, A_1=1\right]-\E\left[Y\mid A_0=0, A_1=0\right]&=\Sexpr{ever}-\Sexpr{never}=\Sexpr{ever-never}\\
\E\left[Y\mid A_0=1,L_1=0, A_1=1\right]-\E\left[Y\mid A_0=0,L_1=0,A_1=0\right]&=\Sexpr{ever_L1eq0}-\Sexpr{never_L1eq0}=\Sexpr{ever_L1eq0-never_L1eq0}\\
\E\left[Y\mid A_0=1,L_1=1, A_1=1\right]-\E\left[Y\mid A_0=0,L_1=0,A_1=0\right]&=\Sexpr{ever_L1eq1}-\Sexpr{never_L1eq1}=\Sexpr{ever_L1eq1-never_L1eq1}
\end{align*}


% %%%%%%%%%%%%%%%%
\subsection{Why traditional methods fail (20.3)}
% %%%%%%%%%%%%%%%%

Stratification can not handle treatment-confounder feedback

In our example in Figure~\ref{fig:20.3}, stratification with respect to $L_1$ is problematic. This, because $L_1$ is a collider for association measure $A_0$ and thus opens the path $A_0\rightarrow L_1 \leftarrow U_1\rightarrow Y$.
This means that stratification induces a non-causal relationship between treatment $A_0$ and unmeasured variable $U_1$ and therefore also between $A_0$ and outcome $Y$, within levels of $L_1$.
Thereby, stratification for $L_1$ eliminates confounding for $A_1$ but introduces \textbf{selection bias} for $A_0$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{G-METHODS FOR TIME-VARYING TREATMENTS (21)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% %%%%%%%%%%%%%%%%
\subsection{The g-formula for time-varying treatments (21.1)}
% %%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Transformation models}\label{sec:trans_model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cite{Hothorn2020} nicely proposes a prospective to unify a wide range of statistical models by moving to conditional distributions and thus leaves the models relying on conditional expectation behind. We get there by rearranging the familiar model, noted by Equation, to model the error term $\bvarepsilon$ as
\begin{align*}
\frac{\y-\boldsymbol{X\beta}}{\sigma}&=\boldsymbol{\varepsilon}
\end{align*}
This is done because the error term $\bvarepsilon$ is the only stochastic component of the model and in this transformed linear model framework we specify the error terms to be standard normally distributed with $\bvarepsilon[i]\sim\N(0,1)$. Moreover, we can treat the constant term from the least-squares method separately by letting $\bbeta=\left[\alpha,\boldsymbol{\tilde{\beta}}\right]$ and $\X=\left[\boldsymbol{1},\boldsymbol{\tilde{X}}\right]$. For one observation, the model takes then the form
\begin{align*}
\frac{\y[i]-\alpha -\boldsymbol{\tilde X}[i,]\boldsymbol{\tilde\beta}}{\sigma}&=\bvarepsilon[i]\sim \N(0,1)
\end{align*}
Modelling via conditional distribution function, this turns to
\begin{align}
\bfP(Y[i]\leq \y[i]\mid\boldsymbol{\tilde X}[i,])=\Phi\left(\frac{\y[i]-\alpha-\boldsymbol{\tilde X}[i,]\boldsymbol{\tilde{\beta}} }{\sigma}\right) \label{eq:lspara}
\end{align}
and to make sense of the name \textit{transformation model} we further reformulate to
\begin{align}
\bfP(Y[i]\leq \y[i]\mid\boldsymbol{\tilde X}[i,])=\Phi\left(\underbrace{-\frac{\alpha}{\sigma}}_{\theta_0}+\underbrace{\frac{1}{\sigma}}_{\theta_1}\y[i]-\boldsymbol{\tilde X}[i,]\underbrace{\frac{\boldsymbol{\tilde\beta}}{\sigma}}_{\boldsymbol{\beta_\tram}}\right)=\Phi\left(\theta_0+\theta_1 \y[i]-\boldsymbol{\tilde X}[i,]\boldsymbol{\beta_\tram} \right) \label{eq:ll_trans}
\end{align}
where we see that the number of parameters to be estimated simultaneously  is now $p+1$ which is due to $\theta_1=\sigma^{-1}$. This means that $\theta_1$ is not estimated independently from $\boldsymbol{\beta_\tram}$ as it is the case in the least-squares setup.

Now, we introduce the transformation function $h(\y[i]|\boldsymbol{\theta})$ which is in this particular case $\theta_0+\theta_1 \y[i]$ and the purpose of it is doing the best it can to transform the response $\y$ to follow the distribution we want, which is here a standard normal distribution $\N(0,1)$ specified by $\Phi(z)=F_Z(z)$:
\begin{align}
\bfP(Y[i]\leq \y[i]\mid\boldsymbol{\tilde X}[i,])=F_Z\left(h(\y[i]|\boldsymbol{\theta})-\boldsymbol{\tilde X}[i,]\boldsymbol{\beta_\tram} \right) \label{eq:gentram}
\end{align}
Equation~\eqref{eq:gentram} describes the general specification of a transformation model as it is used in the \texttt{tram} package \citep{Hothorn2020}. The transformation function in \eqref{eq:ll_trans} is linear and gets fitted by executing the command \texttt{tram::Lm} in \textsf{R}. However, we are by no means limited to this linearity and sometimes it is also necessary to use more complex transformations to assure our model is well-specified. Similarly as we see sometimes log or square-root transformed responses as an attempt to assure normality, we can use highly flexible functions such as splines to get a data-driven transformation. Such functions easily help to transform the outcome, which only has to be at least ordinal, to follow the distribution we want (not limited to normal distribution). The only restriction we must respect is that the transformation function is monotone, not strictly though. Whereas in the linear model so far we have estimated the coefficients via the least-squares method, we estimate them now by optimizing the likelihood. For more details with respect to the underlying functionalities of the \texttt{tram} package we refer the reader to \cite{Hothorn2020} and for more theoretical issues to \cite{Hothorn2017}.

% %%%%%%%%%%%%
\subsection{An example}\label{sec:example}
% %%%%%%%%%%%%

<<echo = F>>=
set.seed(432324)
# true parameters
n <- 50
eps_y <- rnorm(n)
s_y <-  1
beta <- c(4,2,2)
boot <- 10
r1 <- 0.995
r2 <- 0.0
@

Figure~\ref{fig:cond_ill} illustrates what a high and low condition number means in terms of model fitting. Both plots show data that is constructed by a simple equation
\begin{align}
\y=\Sexpr{beta[1]}+\Sexpr{beta[2]}\cdot\X[,1]+\Sexpr{beta[3]}\cdot\X[,2]+\boldsymbol{\varepsilon}\cdot \sigma \label{eq:simplemodel}
\end{align}
where the explanatory variables within $\X$ are $n=\Sexpr{n}$ realizations of a multivariate normal distribution as 
$$X[i,]\sim\N\left(\mu = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \Sigma = \begin{pmatrix} \rho & 0 \\ 0&\rho \end{pmatrix}\right)$$

\begin{figure}[H]%H is strict!
\centering
<<cond,fig.height = 3, fig.width=6,echo=FALSE, eval=T, message=FALSE, warning=FALSE, dev='pdf'>>=
#library("scatterplot3d"); library(scales); library(mvtnorm);library(Collinearity);library(RColorBrewer)
cols <- brewer.pal(9, "YlOrRd")
colfunc <- colorRampPalette(c("red","yellow", "blue"))
cols <- colfunc(12)
#cols <- terrain.colors(10)

par(mfcol = c(1,2))
# High collinearity
set.seed(4324)
X <- mvtnorm::rmvnorm(n=n, mean = c(0,0), sigma = matrix(c(1, r1,r1,1),ncol=2, byrow = T))
colnames(X) <- c("x1","x2")
vdm1 <- Collinearity::Var_decom_mat.matrix(cbind("const"=1,"X"=X))
cond_nu1 <- max(vdm1[,"cond_ind"])
cond_nu10 <- max(Collinearity::Var_decom_mat.matrix(X)[,"cond_ind"])
x1 <- X[,1]; x2 <- X[,2]
y <- beta[1] + x1*beta[2] +x2*beta[3] + eps_y*s_y
dd1 <- data.frame("y"=y, "x1"=x1, "x2"=x2)
m1 <- lm(data = dd1, y ~ x1+x2)

#par(mfcol = c(1,1))
min_z <- -10; max_z <- 15
s3d <- scatterplot3d(x=x1,y=x2,z=y,type ="h",pch = 19, angle = 55, scale.y = 0.6,zlim = c(min_z,max_z),ylim = c(-3,3),
                     ,mar = c(3,3,2,2),
			main = bquote(rho==.(r1)~","~kappa(bold(E) )==~.(cond_nu1) ),cex.lab = 0.8,cex.axis = 0.8,cex.main=0.8,
		zlab=expression(bold(y)),xlab=expression(bold(X)~"[1]"), ylab=expression(bold(X)~"[,2]"))
set.seed(4324)
for(i in 1:boot){
	m_current <- lm(data = dd1[sample(1:nrow(dd1), nrow(dd1), replace = T),], y ~ x1+x2)
	s3d$plane3d(m_current,draw_lines = F,draw_polygon = T,polygon_args = list(border = NA, col = alpha(cols[i], 0.1) ))
	s3d$plane3d(m_current,draw_lines = T,lty.box = "solid", lty = "blank",col = cols[i],lwd = 2)
}
s3d$points3d(x=x1,y=x2,z=rep(min_z,n),pch = 1)

# Low collinearity
set.seed(4324)
X <- mvtnorm::rmvnorm(n=n, mean = c(0,0), sigma = matrix(c(1, r2,r2,1),ncol=2, byrow = T))
colnames(X) <- c("x1","x2")
vdm2 <- Collinearity::Var_decom_mat.matrix(cbind("const"=1,"X"=X))
cond_nu2 <- max(vdm2[,"cond_ind"])
cond_nu20 <- max(Collinearity::Var_decom_mat.matrix(X)[,"cond_ind"])
x1 <- X[,1]; x2 <- X[,2]
y <- beta[1] + x1*beta[2] +x2*beta[3] + eps_y*s_y
dd2 <- data.frame("y"=y, "x1"=x1, "x2"=x2)
m2 <- lm(data = dd2, y ~ x1+x2)

s3d <- scatterplot3d(x=x1,y=x2,z=y,type ="h",pch = 19, angle = 55, scale.y = 0.6,zlim = c(min_z,max_z),mar = c(3,3,2,2),ylim = c(-3,3),
		main = bquote(rho==.(r2)~","~kappa(bold(E) )==~.(cond_nu2) ),cex.lab = 0.8,cex.axis = 0.8,cex.main=0.8,
		zlab=expression(bold(y)),xlab=expression(bold(X)~"[1]"), ylab=expression(bold(X)~"[,2]"))
set.seed(4324)
for(i in 1:boot){
	m_current <- lm(data = dd2[sample(1:nrow(dd2), nrow(dd2), replace = T),], y ~ x1+x2)
	s3d$plane3d(m_current,draw_lines = F,draw_polygon = T,polygon_args = list(border = NA, col = alpha(cols[i], 0.1) ))
	s3d$plane3d(m_current,draw_lines = T,lty.box = "solid", lty = "blank",col = cols[i],lwd = 2)
}
s3d$points3d(x=x1,y=x2,z=rep(min_z,n),pch = 1)
@
\vspace{-7mm}
\caption{Impact of collinearity on the instability of estimates. }\label{fig:cond_ill}
\end{figure}
%\vspace{-5mm}
This allows to tune the amount of collinearity within the system by specifying $\rho$. On the left plot it is chosen to be high with $\rho=\Sexpr{r1}$ and on the right side low with $\rho=\Sexpr{r2}$. By bootstrapping the original sample \Sexpr{boot} times and subsequent model fitting we can visualize the instability that collinearity causes. Because when we plot the planes that represent the area where the models would see $\hy=\hat{\alpha}+\hbbeta[1]\X[,1]+\hbbeta[2]\X[,2]$ we note on the left side with high collinearity ($\kappa\left(\boldsymbol{E}\right)=\Sexpr{cond_nu1}$) that the planes are quite different from each other, whereas on the right side ($\kappa\left(\boldsymbol{E}\right)=\Sexpr{cond_nu2}$) they seem to be very similar and thus stable. Table~\ref{tab:vdm1} shows the corresponding variance decomposition proportion matrices $\bPi$ and the least-squares model results of the original data sets.
\begin{table}[H]
\caption{Variance decomposition matrices as introduced by Belsley in the first row and summary output of the multiple linear regression models on the second row. Left side corresponds to the example with higher collinearity and the right table for the lower.}\label{tab:vdm1}
\begin{subtable}[h]{0.45\textwidth}
\centering
<<vdm1, results = "asis", echo = FALSE, warning=FALSE,message=F>>=
#library(tableone);library(xtable); library(stringr); library(dplyr)
colnames(vdm1) <- str_replace(colnames(vdm1) , "\\^",  "\\\\textasciicircum " )
colnames(vdm1) <- str_replace(colnames(vdm1) , "\\_",  "\\\\_" )
colnames(vdm1)[4:5] <- c("\\textbf{\\textit{X}}[,1]","\\textbf{\\textit{X}}[,2]")
xtab <- xtable( vdm1)
digits(xtab) <- 3
print(xtab, showAllLevels = TRUE, booktabs = TRUE,size = "footnotesize",
      include.rownames=FALSE,include.colnames=TRUE, printToggle = FALSE, noSpaces = TRUE, floating = FALSE,
      hline.after = c(-1,0,nrow(xtab)),
      sanitize.text.function=function(x){x})

@
\end{subtable}
\begin{subtable}[h]{0.45\textwidth}
\centering
<<vdm2, results = "asis", echo = FALSE, warning=FALSE,message=F>>=
vdm1 <- vdm2
colnames(vdm1) <- str_replace(colnames(vdm1) , "\\^",  "\\\\textasciicircum " )
colnames(vdm1) <- str_replace(colnames(vdm1) , "\\_",  "\\\\_" )
colnames(vdm1)[4:5] <- c("\\textbf{\\textit{X}}[,1]","\\textbf{\\textit{X}}[,2]")
xtab <- xtable( vdm1)
digits(xtab) <- 3
print(xtab, showAllLevels = TRUE, booktabs = TRUE,size = "footnotesize",
      include.rownames=FALSE,include.colnames=TRUE, printToggle = FALSE, noSpaces = TRUE, floating = FALSE,
      hline.after = c(-1,0,nrow(xtab)),
      sanitize.text.function=function(x){x})

@
\end{subtable}
%\end{table}
\centering
\begin{subtable}[h]{0.45\textwidth}
\vspace{0.2cm}
<<m1_example, results = "asis",echo=FALSE>>=
#library(biostatUZH);library(xtable)
tableRegression(m1, stats = c("estimate", "standarderror","t.value", "p.value"),
                intercept = T,
                col.nam = c("$\\hat\\beta$", "$\\text{se}\\left(\\hat\\beta\\right)$",
                            "$t$-value", "$p$-value"),
                booktabs = TRUE,floating = F)

@
\end{subtable}
\centering
\begin{subtable}[h]{0.45\textwidth}
\vspace{0.2cm}
<<m2_example, results = "asis",echo=FALSE>>=
tableRegression(m2, stats = c("estimate", "standarderror","t.value", "p.value"),
                intercept = T,
                col.nam = c("$\\hat\\beta$", "$\\text{se}\\left(\\hat\\beta\\right)$",
                            "$t$-value", "$p$-value"),
                booktabs = TRUE,floating = F)
@
\end{subtable}
\end{table}


But why going trough all the trouble with collinear variables and the detrimental effects that come with it and not simply drop one or some of the affected variables? Figure~\ref{fig:coll2} visualizes the model fits when the variable $\X[,2]$ is neglected although truly it has very well an effect on $\y$ as is visible in Equation~\eqref{eq:simplemodel}. We see on the right plot for low collinearity the 95\% confidence interval for $\hbbeta[1]$ does cover the true effect of \Sexpr{beta[2]} whereas for the case with high collinearity this seems to be not the case.
This demonstrates that it is not so easy to simply get rid of some variables as this may introduce bias to some extent.

\begin{figure}[h]%H is strict!
\centering
<<coll2,fig.height = 2.5, fig.width=5,echo=FALSE, eval=T, message=FALSE, warning=FALSE, dev='pdf'>>=
#library(tidyverse)
dd <- rbind(data.frame(dd1, "r" = paste0("rho==",r1,"~'and'~kappa(bold(E))==",cond_nu1) ),
            data.frame(dd2, "r" = paste0("rho==",r2,"~'and'~kappa(bold(E))==",cond_nu2) )
            )
m1 <- lm(data = dd1, y ~ x1)
m2 <- lm(data = dd2, y ~ x1)

# Confidence interval
dd_anno <- rbind(data.frame("label"= paste0("95% CI = [",toString(round(confint(m1)[2,],2)),"]"), 
                            "r" = paste0("rho==",r1,"~'and'~kappa(bold(E))==",cond_nu1) ),
                 data.frame("label"= paste0("95% CI = [",toString(round(confint(m2)[2,],2)),"]"),
                           "r" = paste0("rho==",r2,"~'and'~kappa(bold(E))==",cond_nu2) )
            )

ggplot(data = dd, aes(x = x1, y = y))+
  geom_point()+
  geom_smooth(method = "lm",se = FALSE,col = "black")+
  facet_wrap(.~r,labeller = label_parsed)+
  theme_classic()+
  labs(x = bquote(bold(X)~"[,1]"), y = bquote(bold(y)),title = "Univariate fitted model")+
  geom_label(data = dd_anno, aes(x = -Inf, y = Inf, label  = label,vjust = 1,hjust = 0),size = 3,fill = alpha("black",0.2))
@
\vspace{-5mm}
\caption{Univariate fitted model (\texttt{y$\sim$x1}) of the same data sets as in Figure~\ref{fig:cond_ill}. The slope of the line represents $\hbbeta[1]$ which would be truly \Sexpr{beta[2]} and the confidence interval thereof is given in the box. Obviously, only the right plot with low collinearity seems to capture the true effect whereas with higher collinearity the estimate is biased.}\label{fig:coll2}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
\section{Differences between \texttt{lm} and \texttt{tram::Lm}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The parametrization and the chosen estimation approaches differ between \texttt{lm} and 
\texttt{tram::Lm} and in this section we are going to compare what these differences mean from a theoretical perspective.

% %%%%%%%%%%%%
\subsection{Maximum-Likelihood estimation for the linear regression model}\label{sec:mlnlm}
% %%%%%%%%%%%%

We can show that independent of the estimating procedure, with the parametrization as specified in Equation \eqref{eq:lspara} we will end up at the very same optimization problem if we go over the profile likelihood.
The approximate log-likelihood of a sample that is treated as exact is
\begin{align}
\ell(\boldsymbol{\beta},\sigma|\y)&=-N\log\left(\sigma\right)-\frac{N}{2}\log\left(2\pi\right)-\frac{1}{2}\sum_{i=1}^N\left(-\frac{\alpha}{\sigma}+\frac{1}{\sigma} \y[i]-\boldsymbol{\tilde{X}}[i,]\frac{\boldsymbol{\tilde\beta}}{\sigma}\right)^2 \nonumber\\
&=-N\log\left(\sigma\right)-\frac{N}{2}\log\left(2\pi\right)-\frac{1}{2\sigma^2}\sum_{i=1}^N\left(\y[i]-\alpha-\boldsymbol{\tilde{X}}[i,]\boldsymbol{\tilde\beta}\right)^2 \nonumber\\
&=-N\log\left(\sigma\right)-\frac{N}{2}\log\left(2\pi\right)-\frac{1}{2\sigma^2}\left(\y-\X\bbeta\right)^\top\left(\y-\X\bbeta\right) \label{eq:ll_ls}
\end{align}
We can now employ the profile likelihood where we treat $\sigma$ as the nuisance parameter:
\begin{align*}
\frac{d\ell(\boldsymbol{\beta},\sigma|\y)}{d\sigma}\Big|_{\hat\sigma}=-N\sigma^{-1}+\hat\sigma^{-3}\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)^\top\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)&\stackrel{!}{=}0 \\
\hat\sigma^{-3}\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)^\top\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)&\stackrel{!}{=}N\hat\sigma^{-1}\\
\hat\sigma^2&\stackrel{!}{=}\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)^\top\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)/N
\end{align*}
Plugging $\hat\sigma$ into \eqref{eq:ll_ls}, we see that $\hat\sigma$ vanishes from the equation which is handy:
\begin{align*}
\frac{d\ell(\boldsymbol{\beta},\hat\sigma|\y)}{d\boldsymbol{\beta}}\Big|_{\hbbeta}=-N\log\left(\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)^\top\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)\right)\frac{d}{d\boldsymbol{\beta}}\Big|_{\hbbeta}&\stackrel{!}{=}0\\
\log\left(\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)^\top\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)\right)\frac{d}{d\boldsymbol{\beta}}\Big|_{\hbbeta}&\stackrel{!}{=}0
\end{align*}
Since the log is a monotone function, the maximum likelihood is also found by minimizing the term $\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)^\top\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)$ with respect to $\boldsymbol{\beta}$ and thus the maximum-likelihood estimator $\boldsymbol{\hat\beta}$ is the very same as for the least-squares estimator described in


% %%%%%%%%%%%%
\subsection{Maximum-Likelihood estimation for the transformation model equivalent (\texttt{tram::Lm})}\label{sec:mltramLM}
% %%%%%%%%%%%%

The approximate log-likelihood with the parametrization used for the \texttt{tram::Lm} model specified in Equation~\eqref{eq:ll_trans} is
\begin{align}
\ell(\bbeta_\text{tram},\theta_0,\theta_1|\y)&=+N\log(\theta_1)-\frac{N}{2}\log\left(2\pi\right)-\frac{1}{2}\sum_{i=1}^N\left(\theta_0+\theta_1 \y[i]-\boldsymbol{\tilde{X}}[i,]\bbeta_\text{tram}\right)^2
\label{eq:ll_tramLm}
\end{align}
which has one parameter ($\theta_1$) more to simultaneously estimate.

The design matrix in this setup is different. For \texttt{lm}, $\X$ contained the variables that will be used to explain the outcome $\y$. But this can also be reformulated in terms of using the \textit{variables including the outcome} to explain the  \textit{error} $\bvarepsilon[i]\sim \N(0,1)$.
Since for \texttt{tram::Lm} the parameter $\theta_1$ is attached to the outcome $\y$, the collinearity constellation is not only restricted to the $\X$ space but extends onto $\left[\y,\X\right]$.
This basically implies that the better the outcome $\y$ is explainable by $\X$, the higher the collinearity and thus the larger the effects caused by it. This is important to keep in mind.

\begin{figure}[h]%H is strict!
\begin{center}
<<colllikelihood, fig.height = 3, fig.width = 6,echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE,dev='pdf'>>=
library(tidyverse);library(tram);library(survival)
set.seed(234324)
n <- 100
X <- cbind(rnorm(n =100),rnorm(n =100))
eps_y <- rnorm(n =100)
X <- cbind(1,X)
#beta <- c(2,2,2)#
beta <- c(10,2,2)

# naive loop
res <- c()
s_y <- seq(from = 0, to = 100, length.out = 301)[-1]
for(i in 1:length(s_y)){
  y <- X%*%beta + eps_y*s_y[i]
  data <- data.frame("y"=y, "x1" = X[,2], "x2"=X[,3])
  m_lm <- lm(data = data, y~.)
  m_tram <- tram::Lm(data = data, y~.)
  
  #ms_tram <- tram::Coxph(Surv(y,rep(1, length(data$y)))~x1+x2,data = data)
  #ms_surv <- survival::coxph(Surv(y,rep(1, length(data$y)))~x1+x2,data = data)
  
  
  res <- rbind(res, 
  rbind(
  cbind(names(coef(m_lm))[2:3],summary(m_lm)$coef[2:3,3],"lm",i,s_y[i]),
  cbind(names(coef(m_tram)),summary(m_tram)[["test"]][["test"]][["tstat"]],"tram::Lm",i,s_y[i])
  
  #,cbind(names(coef(ms_tram)),summary(ms_tram)[["test"]][["test"]][["tstat"]],"ztram::Coxph",i,s_y[i])
  #,cbind(names(coef(ms_surv)),summary(ms_surv)$coefficients[,"z"],"zsurival::coxph",i,s_y[i])
  )
  )
}
res <-  data.frame(res)
colnames(res) <- c("variable", "wald", "method", "id","s_y")
rownames(res) <- NULL
res$variable <-  as.factor(res$variable )
res$method <-  as.factor(res$method)
res$wald <-  as.numeric(res$wald)
res$s_y <-  as.numeric(res$s_y)
res$id <-  as.integer(res$id )

# plotting frame
ymax <- 20
res$outlier <- ifelse(res$wald>ymax, 1,0)
res$wald[res$outlier == 1] <- ymax

#plotting
res[res$outlier==0,] %>%
ggplot(aes(x = s_y, y = wald,col = method))+
  geom_line()+
  geom_point(data = res[res$outlier==1,] , aes(x = s_y, y = wald,col = method),
             shape = 17,show.legend = FALSE )+
  facet_grid(.~variable)+
  theme_classic()+
  guides(size = "none" )+
  geom_hline(yintercept = 0)+
  labs(y = "Wald statistics", x = expression(s[y]))
@
\end{center}
\caption{Simulating data as $\y=\Sexpr{beta[1]}+\Sexpr{beta[2]}\x_1+\Sexpr{beta[3]}\x_2+s_y\cdot \bvarepsilon$ with $\left(\x_1,\x_2,\bvarepsilon\right)\sim\N_{3n}(0,1),n=\Sexpr{n}$. The scaling factor $s_y$ is iterated on a grid between \Sexpr{min(s_y)} and \Sexpr{max(s_y)} where a low scaling factor means that the outcome $\y$ is well explainable and thus collinearity for \texttt{tram::Lm} is higher. Wald statistics are plotted restricted to have maximum values of \Sexpr{ymax} and points laying above are illustrated as triangles.}
\label{fig:colllikelihood}
\end{figure}




<<echo=F, eval = F>>=
# anova vs ancova in poisson 
library(mvtnorm)
n <- 100

set.seed(324324)
#U <-runif(n)
beta <- c(2,2,2)
rho <- seq(from = 1/2, to = 20, length.out = 1001)[-1001]
results <- matrix(NA, nrow = length(rho), ncol = 3)

for(i in 1:length(rho)){
  x2 <- rnorm(n = n)
  x1 <- rbinom(n= n, size = 1, prob = plogis(x2*rho[i]))
  
  X <- cbind(1,x1,x2)
  rhosim <- cor(x1,x2)
  
  #print(rhosim)
    #plot(x1,x2)
  
  mu <- exp(X%*%beta)
  y <- rpois(n = n, lambda = mu)
  dd <- data.frame("y" = y, "x1" = x1, "x2" = x2)

  m1 <- glm(data = dd, family = poisson(link = log), y~x1)
  se1 <- sqrt(diag(vcov(m1))["x1"])
  m2 <- glm(data = dd, family = poisson(link = log), y~x1+x2)
  se2 <- sqrt(diag(vcov(m2))["x1"])

  results[i,] <- c("se1" = se1, "se2" = se2, "rhosim" = rhosim)
}

plot(x = results[,3],y = results[,1])
lines(x = results[,3],y = results[,2],col = "blue",type = "p")



@


